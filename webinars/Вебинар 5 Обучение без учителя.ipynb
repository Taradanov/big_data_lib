{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вебинар 5. Обучение без учителя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Введение\" data-toc-modified-id=\"Введение-1\">Введение</a></span></li><li><span><a href=\"#Подключение-библиотек-и-скриптов\" data-toc-modified-id=\"Подключение-библиотек-и-скриптов-2\">Подключение библиотек и скриптов</a></span></li><li><span><a href=\"#Загрузка-данных\" data-toc-modified-id=\"Загрузка-данных-3\">Загрузка данных</a></span></li><li><span><a href=\"#Разбиение-на-train-и-test\" data-toc-modified-id=\"Разбиение-на-train-и-test-4\">Разбиение на train и test</a></span></li><li><span><a href=\"#Масштабирование-признаков\" data-toc-modified-id=\"Масштабирование-признаков-5\">Масштабирование признаков</a></span></li><li><span><a href=\"#Понижение-размерности:-PCA---Метод-главных-компонент\" data-toc-modified-id=\"Понижение-размерности:-PCA---Метод-главных-компонент-6\">Понижение размерности: PCA - Метод главных компонент</a></span><ul class=\"toc-item\"><li><span><a href=\"#Снижение-размерности-до-2х-компонент\" data-toc-modified-id=\"Снижение-размерности-до-2х-компонент-6.1\">Снижение размерности до 2х компонент</a></span></li><li><span><a href=\"#Снижение-размерности-до-3х-компонент\" data-toc-modified-id=\"Снижение-размерности-до-3х-компонент-6.2\">Снижение размерности до 3х компонент</a></span></li><li><span><a href=\"#Использование-результатов-PCA-в-построении-прогнозной-модели\" data-toc-modified-id=\"Использование-результатов-PCA-в-построении-прогнозной-модели-6.3\">Использование результатов PCA в построении прогнозной модели</a></span></li></ul></li><li><span><a href=\"#Кластеризация:-k-means---Метод-k-средних\" data-toc-modified-id=\"Кластеризация:-k-means---Метод-k-средних-7\">Кластеризация: k-means - Метод k-средних</a></span><ul class=\"toc-item\"><li><span><a href=\"#Оценка-оптимального-количества-кластеров\" data-toc-modified-id=\"Оценка-оптимального-количества-кластеров-7.1\">Оценка оптимального количества кластеров</a></span></li><li><span><a href=\"#Выделение-3-кластеров\" data-toc-modified-id=\"Выделение-3-кластеров-7.2\">Выделение 3 кластеров</a></span></li><li><span><a href=\"#Выделение-6-кластеров\" data-toc-modified-id=\"Выделение-6-кластеров-7.3\">Выделение 6 кластеров</a></span></li><li><span><a href=\"#Использование-результатов-кластеризации-в-построении-прогнозной-модели\" data-toc-modified-id=\"Использование-результатов-кластеризации-в-построении-прогнозной-модели-7.4\">Использование результатов кластеризации в построении прогнозной модели</a></span></li><li><span><a href=\"#Оценка-финальной-модели\" data-toc-modified-id=\"Оценка-финальной-модели-7.5\">Оценка финальной модели</a></span></li></ul></li><li><span><a href=\"#Задание-для-курсового-проекта\" data-toc-modified-id=\"Задание-для-курсового-проекта-8\">Задание для курсового проекта</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Кластеризация** (сегментирование данных для упрощения обработки, выделение нетипичных объектов)\n",
    "\n",
    "2. **Понижение размерности** (визуализация, сжатие информации, получение новых признаков)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подключение библиотек и скриптов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_elbow_method(X):\n",
    "    distortions = []\n",
    "    K = range(2,15)\n",
    "    for k in K:\n",
    "        kmeanModel = KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "        distortions.append(sum(np.min(cdist(X, \n",
    "                                            kmeanModel.cluster_centers_, \n",
    "                                            'euclidean'), \n",
    "                                      axis=1)) / X.shape[0])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method showing the optimal k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_clusters_distribution(unique_labels, labels_counts):\n",
    "    plt.figure(figsize=(8,5))\n",
    "\n",
    "    plt.bar(unique, counts)\n",
    "\n",
    "    plt.xlabel('Clusters')\n",
    "    plt.xticks(unique)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Clusters distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dims_to_2D_space_with_PCA(df):\n",
    "    pca = PCA(n_components=2)\n",
    "    components = pca.fit_transform(df)\n",
    "    return pd.DataFrame(data = components, columns = ['component_1', 'component_2'])\n",
    "    \n",
    "def reduce_dims_to_3D_space_with_PCA(df):\n",
    "    pca = PCA(n_components=3)\n",
    "    components = pca.fit_transform(df)\n",
    "    return pd.DataFrame(data = components, columns = ['component_1', 'component_2', 'component_3'])\n",
    "\n",
    "def reduce_dims_to_2D_space_with_TSNE(df):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    components = tsne.fit_transform(df)\n",
    "    return pd.DataFrame(data = components, columns = ['component_1', 'component_2'])\n",
    "    \n",
    "def reduce_dims_to_3D_space_with_TSNE(df):\n",
    "    tsne = TSNE(n_components=3, random_state=42)\n",
    "    components = tsne.fit_transform(df)\n",
    "    return pd.DataFrame(data = components, columns = ['component_1', 'component_2', 'component_3'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_components_in_3D_space(components_df, labels=None):\n",
    "    components_with_labels_df = pd.concat([components_df, labels], axis=1)\n",
    "\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    if labels is not None:\n",
    "        if labels.nunique() > 10:\n",
    "            p = ax.scatter(components_with_labels_df['component_1'], \n",
    "                       components_with_labels_df['component_2'], \n",
    "                       components_with_labels_df['component_3'], \n",
    "                       c=labels, cmap=plt.get_cmap('jet'), alpha=0.5)\n",
    "        else:\n",
    "             p = ax.scatter(components_with_labels_df['component_1'], \n",
    "                       components_with_labels_df['component_2'], \n",
    "                       components_with_labels_df['component_3'], \n",
    "                       c=labels, cmap=plt.get_cmap('jet', labels.nunique()), alpha=0.5)           \n",
    "    else:\n",
    "        p = ax.scatter(components_with_labels_df['component_1'], \n",
    "                   components_with_labels_df['component_2'], \n",
    "                   components_with_labels_df['component_3'], \n",
    "                   alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel('component_1')\n",
    "    ax.set_ylabel('component_2')\n",
    "    ax.set_zlabel('component_3')\n",
    "    ax.set_title('3D mapping of objects')\n",
    "    fig.colorbar(p);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_components_in_2D_space(components_df, labels=None):\n",
    "    components_with_labels_df = pd.concat([components_df, labels], axis=1)\n",
    "    \n",
    "    if labels is not None:\n",
    "        if labels.nunique() > 10:\n",
    "            p = components_with_labels_df.plot(kind='scatter', x='component_1', y='component_2', \n",
    "                                             c=labels.name, cmap=plt.get_cmap('jet'),\n",
    "                                             alpha=0.5, figsize=(12,6), sharex=False)\n",
    "        else:\n",
    "            p = components_with_labels_df.plot(kind='scatter', x='component_1', y='component_2', \n",
    "                                             c=labels.name, cmap=plt.get_cmap('jet', labels.nunique()),\n",
    "                                             alpha=0.5, figsize=(12,6), sharex=False)   \n",
    "    else:\n",
    "        p = components_with_labels_df.plot(kind='scatter', x='component_1', y='component_2', \n",
    "                                       alpha=0.5, figsize=(12,6))\n",
    "\n",
    "    plt.xlabel('component_1')\n",
    "    plt.ylabel('component_2')\n",
    "    plt.title('2D mapping of objects'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(true_values_train, pred_values_train, true_values_test, pred_values_test):\n",
    "    print('Train:\\t' + 'R2 = ' + str(round(r2_score(true_values_train, pred_values_train), 4)) + \n",
    "          '\\tMAE = ' + str(round(mean_absolute_error(true_values_train, pred_values_train), 3)) +\n",
    "          '\\n' +\n",
    "          'Test:\\t' + 'R2 = ' + str(round(r2_score(true_values_test, pred_values_test), 4)) +\n",
    "          '\\tMAE = ' + str(round(mean_absolute_error(true_values_test, pred_values_test), 3))\n",
    "         )\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))\n",
    "       \n",
    "    plt.subplot(121)\n",
    "    sns.scatterplot(x=pred_values_train, y=true_values_train)\n",
    "    plt.plot([0, 500000], [0, 500000], linestyle='--', color='black')\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('True values')\n",
    "    plt.title('Train: True vs Predicted values');\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    sns.scatterplot(x=pred_values_test, y=true_values_test)\n",
    "    plt.plot([0, 500000], [0, 500000], linestyle='--', color='black')\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('True values')\n",
    "    plt.title('Test: True vs Predicted values');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пути к директориям и файлам**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = '../data/housing_prepared.csv'\n",
    "#TRAIN_DATASET_PATH = '../data/housing_train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Описание датасета**\n",
    "\n",
    "Статистические данные о группах домов в Калифорнии, основанные на переписи 1990 года.   \n",
    "\n",
    "A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
    "\n",
    "* **longitude** - долгота группы\n",
    "* **latitude** - широта группы\n",
    "* **housing_median_age** - средний возраст дома в группе (Median age of a house within a block)\n",
    "* **total_rooms** - общее количество комнат в группе домов (Total number of rooms within a block)\n",
    "* **total_bedrooms** - общее количество спален в группе домов (Total number of bedrooms within a block)\n",
    "* **population** - количество проживающих в группе домов (Total number of people residing within a block)\n",
    "* **households** - количество семей (Total number of households, a group of people residing within a home unit, for a block)\n",
    "* **ocean_proximity** - близость океана\n",
    "* **median_income** - медианный доход семьи (Median income for households within a block of houses, measured in tens of thousands of US Dollars)\n",
    "* **median_house_value** - медианная стоимость дома (Median house value for households within a block, measured in US Dollars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_DATASET_PATH)\n",
    "df.drop(['ocean_proximity'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение на train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['median_house_value'], axis=1)\n",
    "y = df['median_house_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    shuffle=True, \n",
    "                                                    random_state=42)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Масштабирование признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Понижение размерности: PCA - Метод главных компонент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Как работает PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MinMaxScaler() для каждого значения x заменяет: (x - min) / (max - min)\n",
    "# StandardScaler() для каждого значения x заменяет: (x - mean) / std\n",
    "# RobustScaler() для каждого значения x заменяет: (x - median) / (q75 - q25)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train),\n",
    "                        columns=X_train.columns,\n",
    "                        index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test),\n",
    "                        columns=X_test.columns,\n",
    "                        index=X_test.index)\n",
    "\n",
    "features = X_train_scaled.iloc[:, :8]\n",
    "features.describe().T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](PCA.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Первая главная компонента:\n",
    "\n",
    "$$\\text{PC1}=a_1X_A+b_1X_B$$\n",
    "\n",
    "Вторая главная компонента:\n",
    "\n",
    "$$\\text{PC2}=a_2X_A+b_2X_B$$\n",
    "\n",
    "где $a_i, b_i$ - веса переменных $X_A$ и $X_B$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Снижение размерности до 2х компонент"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "components_2d_pca = reduce_dims_to_2D_space_with_PCA(features)\n",
    "components_2d_tsne = reduce_dims_to_2D_space_with_TSNE(features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_components_in_2D_space(components_2d_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_components_in_2D_space(components_2d_tsne, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Снижение размерности до 3х компонент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_3d_pca = reduce_dims_to_3D_space_with_PCA(features)\n",
    "components_3d_tsne = reduce_dims_to_3D_space_with_TSNE(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_components_in_3D_space(components_3d_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_components_in_3D_space(components_3d_tsne, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование результатов PCA в построении прогнозной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, насколько PCA улучшит результаты для предсказательной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3, random_state=42)\n",
    "components = pca.fit_transform(features)\n",
    "components_3d_pca_train = pd.DataFrame(data = components, \n",
    "                                      columns = ['component_1', 'component_2', 'component_3'],\n",
    "                                      index=features.index)\n",
    "\n",
    "for i, component in enumerate(pca.components_):\n",
    "    print(\"{} component: {}% of initial variance\".format(i + 1, \n",
    "          round(100 * pca.explained_variance_ratio_[i], 2)))\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name)\n",
    "                     for value, name in zip(component,\n",
    "                                            features.columns)), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_3d_pca_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.transform(X_test_scaled.iloc[:, :8])\n",
    "components_3d_pca_test = pd.DataFrame(data = components, \n",
    "                                      columns = ['component_1', 'component_2', 'component_3'],\n",
    "                                      index=X_test_scaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pd.concat([X_train, \n",
    "                   components_3d_pca_train], axis=1)\n",
    "X_train.shape, X_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca = pd.concat([X_test, \n",
    "                   components_3d_pca_test], axis=1)\n",
    "X_test.shape, X_test_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingRegressor(n_estimators=100, max_depth=7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 вариант**: обучим модель на данных без учета результатов PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = gb_model.predict(X_train)\n",
    "y_test_preds = gb_model.predict(X_test)\n",
    "evaluate_preds(y_train, y_train_preds, y_test, y_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 вариант**: обучим модель на данных с учетом PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = gb_model.predict(X_train_pca)\n",
    "y_test_preds = gb_model.predict(X_test_pca)\n",
    "evaluate_preds(y_train, y_train_preds, y_test, y_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(zip(X_train_pca.columns, gb_model.feature_importances_), \n",
    "                                   columns=['feature_name', 'importance'])\n",
    "\n",
    "feature_importances.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация: k-means - Метод k-средних"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Как работает k-means**\n",
    "\n",
    "Вы указываете кол-во кластеров\n",
    "\n",
    "1. Центры кластеров случайно инициализируются\n",
    "2. От каждой точки рассчитывается расстояние до центра каждого кластера\n",
    "3. Присваиваем каждую точку к тому кластеру, к центру которого она ближе\n",
    "4. Пересчитываем центры кластеров: берем среднее по всем фичам --> новый центр кластера\n",
    "    \n",
    "итеративно повторяем шаги 2-4 пока центры кластеров не перестанут \"двигаться\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"kmeans_v2.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка оптимального количества кластеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "apply_elbow_method(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделение 3 кластеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_3 = KMeans(n_clusters=3, random_state=42)\n",
    "labels_clast_train_3 = kmeans_3.fit_predict(features)\n",
    "labels_clast_train_3 = pd.Series(labels_clast_train_3, name='clusters_3')\n",
    "\n",
    "unique, counts = np.unique(labels_clast_train_3, return_counts=True)\n",
    "display_clusters_distribution(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_components_in_2D_space(components_2d_pca, labels_clast_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_components_in_3D_space(components_3d_pca, labels_clast_train_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделение 6 кластеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_6 = KMeans(n_clusters=6, random_state=42)\n",
    "labels_clast_train_6 = kmeans_6.fit_predict(features)\n",
    "labels_clast_train_6 = pd.Series(labels_clast_train_6, name='clusters_6')\n",
    "\n",
    "unique, counts = np.unique(labels_clast_train_6, return_counts=True)\n",
    "display_clusters_distribution(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_components_in_2D_space(components_2d_pca, labels_clast_train_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_components_in_3D_space(components_3d_pca, labels_clast_train_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование результатов кластеризации в построении прогнозной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим новые признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ext = pd.concat([X_train_pca.reset_index(), \n",
    "                        labels_clast_train_3, \n",
    "                        labels_clast_train_6], axis=1)\n",
    "\n",
    "X_train_ext.set_index('index', inplace=True)\n",
    "X_train_pca.shape, labels_clast_train_3.shape, labels_clast_train_6.shape, X_train_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_clast_test_3 = pd.Series(kmeans_3.predict(X_test_scaled.iloc[:, :8]), name='clusters_3')\n",
    "labels_clast_test_6 = pd.Series(kmeans_6.predict(X_test_scaled.iloc[:, :8]), name='clusters_6')\n",
    "\n",
    "X_test_ext = pd.concat([X_test_pca.reset_index(), \n",
    "                        labels_clast_test_3, \n",
    "                        labels_clast_test_6], axis=1)\n",
    "\n",
    "X_test_ext.set_index('index', inplace=True)\n",
    "X_test_pca.shape, labels_clast_test_3.shape, labels_clast_test_6.shape, X_test_ext.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка финальной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ext.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gb_model = GradientBoostingRegressor(n_estimators=100, max_depth=7, random_state=42)\n",
    "gb_model.fit(X_train_ext, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = gb_model.predict(X_train_ext)\n",
    "y_test_preds = gb_model.predict(X_test_ext)\n",
    "evaluate_preds(y_train, y_train_preds, y_test, y_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(zip(X_train_ext.columns, gb_model.feature_importances_), \n",
    "                                   columns=['feature_name', 'importance'])\n",
    "\n",
    "feature_importances.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train_ext, y_train], axis=1)\n",
    "target_gr_by_clast = train.groupby('clusters_3')['median_house_value'].mean()\n",
    "\n",
    "train = train.merge(target_gr_by_clast, how='left', on='clusters_3', suffixes=('', '_clust'))\n",
    "train.drop('median_house_value', axis=1, inplace=True)\n",
    "\n",
    "test = X_test_ext.merge(target_gr_by_clast, how='left', on='clusters_3', suffixes=('', '_clust'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model.fit(train, y_train)\n",
    "y_test_preds = gb_model.predict(test)\n",
    "round(r2_score(y_test, y_test_preds), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(zip(train.columns, gb_model.feature_importances_), \n",
    "                                   columns=['feature_name', 'importance'])\n",
    "\n",
    "feature_importances.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_list = feature_importances.nlargest(17, 'importance')['feature_name'].to_list()\n",
    "gb_model.fit(train[feature_imp_list], y_train)\n",
    "y_test_preds = gb_model.predict(test[feature_imp_list])\n",
    "round(r2_score(y_test, y_test_preds), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание для курсового проекта \n",
    "\n",
    "\n",
    "На уроке 10 (\"Вебинар. Консультация по итоговому проекту\") разберем пример решения курсовой работы. См здесь - https://www.kaggle.com/irinatel/baseline-0520\n",
    "\n",
    "\n",
    "Метрика:\n",
    "R2 - коэффициент детерминации (sklearn.metrics.r2_score)\n",
    "\n",
    "Сдача проекта:\n",
    "1. Результаты собираются до **12.10.21 20:00**. 13 октября рейтинг будет выставлен в разделе Обсуждения Урока 10.\n",
    "2. Прислать в раздел Задания Урока 10 ссылки на **ноутбук** и на **файл с предсказаниями**.\n",
    "3. Необходимо получить **R2 > 0.65**.\n",
    "\n",
    "\n",
    "Примечание:\n",
    "Файл с предсказанными ценами для квартир из test.csv назвать по образцу YourName_predictions_group.csv. Он должен содержать два поля (Id, Price) и 5001 строку (шапка + 5000 предсказаний).\n",
    "____________\n",
    "Рекомендации для файла с кодом (YourName_solution_group.ipynb):\n",
    "1. Весь проект должен быть в одном файле\n",
    "2. Файл должен содержать заголовки и комментарии\n",
    "3. Повторяющиеся операции лучше оформлять в виде функций\n",
    "4. По возможности добавлять графики, описывающие данные \n",
    "5. Оставить только лучшую модель, то есть не включать в код все варианты решения проекта\n",
    "6. Проект должен полностью отрабатывать за разумное время (не больше 10 минут),\n",
    "поэтому в финальный вариант лучше не включать GridSearch с перебором большого количества сочетаний параметров, а сразу использовать подобранные параметры\n",
    "7. При использовании статистик (среднее, медиана и т.д.) в качестве признаков, считать их на трейне и использовать на контрольных и тестовых данных\n",
    "8. Оценку качества модели можно выполнять на отложенной выборке или использовать кросс-валидацию. Также рекомендуется самостоятельно проверять точность предсказания на https://www.kaggle.com/c/realestatepriceprediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Содержание",
   "toc_cell": true,
   "toc_position": {
    "height": "345.6px",
    "left": "427px",
    "top": "134.6px",
    "width": "165px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}